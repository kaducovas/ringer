{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ringer framework: Tuning Tools package\n",
    "\n",
    "This package contains all tools used for tuning and exporting the discriminators into the Athena/RootCore environment. It is integrated with CERN grid when with panda access, so that the discriminators can be tuned both on the CERN grid or on standalone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "This package cannot be installed by itself. Consider installing one of the RootCore projects using this package:\n",
    "\n",
    " - [RingerTuning](https://github.com/wsfreund/RingerTuning) [recommended]: this project contains only the packages needed for tuning the discriminators;\n",
    " - [RingerProject](https://github.com/joaoVictorPinto/RingerProject): Use this git repository, however, if you want to install all packages related to the Ringer algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Cores\n",
    "\n",
    "There are two cores that can be used for tuning the discriminators. The available cores are defined during the compile time. To achieve a faster compilation (and less disk space usage), you can require installation of just one of the cores. The flags available when compiling with `buildthis.sh` are the following:\n",
    "\n",
    "- `--with-tuningtool-fastnet` (default): Uses an adapted version of the [FastNet](https://github.com/rctorres/fastnet); \n",
    "- `--with-tuningtool-exmachina`: Install [ExMachina](https://github.com/Tiamaty/ExMachina) as its core;\n",
    "- `--with-tuningtool-all`: Install both cores. The core to be used to tune can be specified during Runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other compile time flags\n",
    "\n",
    "Currently, it is available the following flag:\n",
    "\n",
    "- `--with-tuningtool-dbg-level`: When specified and compiling fastnet, it will be compiled on debug mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "Usually, for every functionality available on this package, you will both be able to access it through shell commands (available after you set the project environment through `source setrootcore.sh`), or through a python script. For the latter, we have created skeletons on the folder [TuningTools/scripts/skeletons/](https://github.com/wsfreund/TuningTools/tree/master/scripts/skeletons/) which can be used as tutorial examples and changed to your needs.\n",
    "\n",
    "The next steps describe the usual work flow. Steps marked with the [GRID] flag can be skipped if you are not going to run the tuning on the CERN grid, although steps concerning data configuration can still be used for tuning on standalone:\n",
    "\n",
    "1. Transform data either in PhysVal or xAOD (in upcoming version) format to the package data format. Take a look at [\"Creating Data\" documentation](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/CreateData.ipynb).\n",
    "1. [GRID] Generate the tuning configuration data. Take a look at [\"Tuning the Discriminator\" documentation](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/Tuning.ipynb#Create-Configuration-Data).\n",
    "1. [GRID] Export datasets to the grid. Take a look at [\"Tuning the Discriminator\" documentation](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/Tuning.ipynb#Exporting-data-to-the-GRID). \n",
    "1. Run the tuning:\n",
    "    1. [GRID] Use the [runGRIDtuning.py](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/Tuning.ipynb#Tuning-on-the-GRID) command;\n",
    "    1. [standalone] Use the [runTuning.py](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/Tuning.ipynb#Tuning-standalone) command.\n",
    "1. Retrieve the Cross-Validation statistics. Take a look at [\"Cross-Validation Statistics Retrieval\" documentation](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/CrossValStats.ipynb);\n",
    "1. Dump the operational discriminator for usage on physics reconstruction/trigger environment Take a look at [\"Cross-Validation Statistics Retrieval\" documentation](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/CrossValStats.ipynb#Dumping-operational-discriminator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package is organized as a standard RootCore package, namely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module '/afs/cern.ch/user/w/wsfreund/Ringer/xAODRingerOfflinePorting/RingerTPFrameWork/TuningTools' folders are:\n",
      "./Root\n",
      "./cmt\n",
      "./python\n",
      "./TuningTools\n",
      "./scripts\n",
      "./doc\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Module '$(pwd)' folders are:\"\n",
    "find -L . -type d -maxdepth 1 -not -name \".*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cmt` folder only matters for the developers. On the `Root` folder we only generate the dictionary for the PhysVal ROOT TTree, which is set on the [`TuningTools/RingerPhysVal.h`](https://github.com/wsfreund/TuningTools/tree/master/TuningTools/RingerPhysVal.h).\n",
    "\n",
    "The user interaction will happen mainly with `python` and `scripts` folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python files\n",
    "\n",
    "When checking `python` folder, we will see the following modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./python/CreateData.py\n",
      "./python/CreateTuningJobFiles.py\n",
      "./python/CrossValid.py\n",
      "./python/CrossValidStat.py\n",
      "./python/ReadData.py\n",
      "./python/Neural.py\n",
      "./python/Parser.py\n",
      "./python/PreProc.py\n",
      "./python/TuningJob.py\n",
      "./python/__init__.py\n",
      "./python/TuningWrapper.py\n",
      "./python/coreDef.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "find -L ./python -maxdepth 2 -mindepth 1 -not -name \"*.pyc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the main purposes are the following:\n",
    "\n",
    " - [`python/ReadData.py`](https://github.com/wsfreund/TuningTools/tree/master/python/ReadData.py): It can be considered as an implementation detail for the TuningTools data files creation. Its main class `ReadData` is internally used by the data creation routine which is prefered rather than directly using the ReadData. However, documentation on the `ReadData` usage is also available [here](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/CreateData.ipynb#Using-ReadData). In this file you will find the `BranchEffCollector` and  `BranchCrossEffCollector` which are the classes used to store the benchmark efficiencies on the tuning data files. Many important enumerations can be found on this file, which are extensively used on other module files. The most used enumerations are:\n",
    "     - Dataset: defines which Cross-Validation dataset the data is in;\n",
    "     - RingerOperation: defines where the *Ringer* algorithm is operating (which Trigger level or Offline);\n",
    "     - Reference: defines which benchmark (*Truth*, *Likelihood* or *CutBased*) should be used as reference for filtering the particles.\n",
    " - [`python/CreateData.py`](https://github.com/wsfreund/TuningTools/tree/master/python/CreateData.py): Its main class `CreateData` is used for creating the module used data files. The structure of this file is defined on `TuningDataArchieve` the tuning data archive, also responsible for managing these files loading and saving;\n",
    " - [`python/CreateTuningJobFiles.py`](https://github.com/wsfreund/TuningTools/tree/master/python/CreateTuningJobFiles.py): Contains the GRID configuration files which basically contains simple information such as which tuning configurations should be used and which CrossValidation sort will go on each job. Usually each file will be a job on the GRID, however the tuning job can run more than one file if you want to complicate things a little bit. On this file you will also find the `TuningJobConfigArchieve`, which is the context manager for the tuning configuration files;\n",
    " - [`python/CrossValid.py`](https://github.com/wsfreund/TuningTools/tree/master/python/CrossValid.py): The Cross-Validation manager. Its main class `CrossValid` contains all box sorts for each k-fold and can be used for applying and reverting the sort into the data. The user should not care about the implementation details as this is handled by the `TuningJob`. This file also contains the context manager `CrossValidArchieve` for saving and loading the Cross-Validation data files which are needed to run tuning jobs on the GRID; \n",
    " - [`python/PreProc.py`](https://github.com/wsfreund/TuningTools/tree/master/python/PreProc.py): Defines the pre-processing algorithms which can be applied on the data. It is possible to apply more than one pre-processing by using the `PreProcChain`. If you want the tuning job to tune more than one pre-processing chain, you can create a pre-processing chains collection with the `PreProcCollection` class. This file also contains the `PreProcArchieve` which is the context manager for saving and loading the pre-processing data.\n",
    " - [`python/Parser.py`](https://github.com/wsfreund/TuningTools/tree/master/python/Parser.py): On this file you will find several parsers definitions which are used by the executables located on the `scripts` folder;\n",
    " - [`python/TuningJob.py`](https://github.com/wsfreund/TuningTools/tree/master/python/TuningJob.py): Its main class `TuningJob` handles all data and configuration for the user, correctly calling the core algorithm to tune the discriminators. The results are saved on `TunedDiscrArchieve` format, which are loaded using the same class as a context manager;\n",
    " - [`python/TuningTool.py`](https://github.com/wsfreund/TuningTools/tree/master/python/TuningTool.py): Contains a wrapper for the tuning core;\n",
    " - [`python/Neural.py`](https://github.com/wsfreund/TuningTools/tree/master/python/TuningTool.py): Contains a wrapper for the NeuralNetwork class returned by the tuning core;\n",
    " - [`python/coreDef.py`](https://github.com/wsfreund/TuningTools/tree/master/python/coreDef.py): Defines the cores used for tuning available and provides runtime mechanism for selecting which one of them to use;\n",
    " - [`python/CrossValidStat.py`](https://github.com/wsfreund/TuningTools/tree/master/python/CrossValidStat.py): Its main class is used to retrieve the CrossValidation statistics on the chosen operating points. This returns a summary operation dictionary (also saved in a file) which can be further used to dump the operation discriminator. The `ReferenceBenchmark` class is used to retrieve the discriminators efficiency on the operating points and the `PerfHolder` contains the discriminators tuning performance information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script files\n",
    "\n",
    "The most important content for the users are defined within the scripts folder. Instead of interacting with the `python` folder, the user can run the package functionalities by running directly shell executables defined on the `scripts/standalone` (runs the functionalities on standalone) and `scripts/grid_scripts` (on the GRID if applicable) folders. Another important folder is the `scripts/skeletons` where skeletons for interacting with the python packages can be found.\n",
    "\n",
    "All scripts folder are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./scripts/grid_scripts\n",
      "./scripts/run_on_grid\n",
      "./scripts/standalone\n",
      "./scripts/validate\n",
      "./scripts/analysis_scripts\n",
      "./scripts/skeletons\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "find -L ./scripts -mindepth 1 -maxdepth 1 -not -name \"*.pyc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scripts/validate` folder have validation scripts, and the `scripts/run_on_grid` contains scripts which are run internally inside the GRID. Finally, the `scripts/analysis_scripts` folder contain past analysis/tuning used scripts, users are encouraged to keep their scripts on this folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving help on the python executables\n",
    "\n",
    "You might have issues when trying to retrieve help when running the executable python commands, as the -h flag is read first the python itself. To bypass python options, add first a `--` before the commands and then add `-h`. E.g.:\n",
    "\n",
    "```\n",
    "createData.py -- -h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone\n",
    "\n",
    "All standalone scripts found in this package are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./scripts/standalone/createData.py\n",
      "./scripts/standalone/createTuningJobFiles.py\n",
      "./scripts/standalone/filterTree.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "find -L ./scripts/standalone -mindepth 1 -maxdepth 1 -not -name \"*.pyc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where a brief description about their utility is:\n",
    "\n",
    " - [`scripts/standalone/createData.py`](https://github.com/wsfreund/TuningTools/tree/master/scripts/standalone/createData.py): Execute the needed information extraction for tuning the discriminators from the xAOD/PhysVal files. For more information see the [Creating Data documentation](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/CreateData.ipynb#Using-the-createData.py-executable);\n",
    " - [`scripts/standalone/createTuningJobFiles.py`](https://github.com/wsfreund/TuningTools/tree/master/scripts/standalone/createTuningJobFiles.py): generate all tuning job configuration files, as the looping bounds for each job, the pre-processing chains and the Cross-Validation file;\n",
    " - [`scripts/standalone/filterTree.py`](https://github.com/wsfreund/TuningTools/tree/master/scripts/standalone/filterTree.py): skim PhysVal files to contain only a small number of trigger chains. Use this script if you already have the PhysVal downloaded, if you still need to download it, the `scripts/grid_scripts/run_dump.py` better fits your need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID\n",
    "\n",
    "Now entering in details about the executables which send jobs to the GRID, the available scripts are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./scripts/grid_scripts/add_container.sh\n",
      "./scripts/grid_scripts/createGRIDTuningJobFiles.py\n",
      "./scripts/grid_scripts/genGRIDdata.py\n",
      "./scripts/grid_scripts/retryBSUBtuning.py\n",
      "./scripts/grid_scripts/runBSUBtuning.py\n",
      "./scripts/grid_scripts/runBSUBtuning.sh\n",
      "./scripts/grid_scripts/runGRIDtuning.py\n",
      "./scripts/grid_scripts/run_dump.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "find -L ./scripts/grid_scripts -mindepth 1 -maxdepth 1 -not -name \"*.pyc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where a brief explanation about their utility is:\n",
    "- [`scripts/grid_scripts/add_container.sh`](https://github.com/wsfreund/TuningTools/tree/master/scripts/grid_scripts/add_container.sh): A shell script used for uploading data to the GRID. It must be used to upload all locally available data and configuration data to the GRID, including the pre-processing and Cross-Validation data. The development of jobs generating those information directly on the GRID is under-development. For more information, take a look at [\"Tuning the Discriminator\" documentation](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/Tuning.ipynb#Uploading-data-to-the-GRID);\n",
    "- [`scripts/grid_scripts/createGRIDTuningJobFiles.py`](https://github.com/wsfreund/TuningTools/tree/master/scripts/grid_scripts/createGRIDTuningJobFiles.py): Under development. It will make possible to generate the configuration files directly on the GRID;\n",
    "- [`scripts/grid_scripts/genGRIDdata.py`](https://github.com/wsfreund/TuningTools/tree/master/scripts/grid_scripts/genGRIDdata.py): Under development. It will make possible to generate the data file directly on the GRID;\n",
    "- [`scripts/grid_scripts/retryBSUBtuning.py`](https://github.com/wsfreund/TuningTools/tree/master/scripts/grid_scripts/retryBSUBtuning.py): Retry failed jobs on LSF;\n",
    "- [`scripts/grid_scripts/runBSUBtuning.py`](https://github.com/wsfreund/TuningTools/tree/master/scripts/grid_scripts/runBSUBtuning.py): Run jobs on LSF queues;\n",
    "- [`scripts/grid_scripts/runGRIDtuning.py`](https://github.com/wsfreund/TuningTools/tree/master/scripts/grid_scripts/runGRIDtuning.py): Run job on the CERN grid. Take a look at [\"Tuning the Discriminator\" documentation](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/Tuning.ipynb#Running-the-GRID-dispatch-tuning-command);\n",
    "- [`scripts/grid_scripts/run_dump.py`](https://github.com/wsfreund/TuningTools/tree/master/scripts/grid_scripts/run_dump.py): Download and optimizes PhysVals from the GRID in batches so that only the desired chains to reduce disk-space usage. For more information, take a look at [\"Creating Data\" documentation](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/tree/master/doc/CreateData.ipynb#Optimal-GRID-PhysVal-download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\">\n",
    "    show=true;\n",
    "    function toggle(){\n",
    "        if (show){\n",
    "            $('div.input').hide();\n",
    "        }else{\n",
    "            $('div.input').show();\n",
    "        }\n",
    "        show = !show\n",
    "    }\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
    "</script>\n",
    "<a href=\"javascript:toggle()\" target=\"_self\"></a>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
