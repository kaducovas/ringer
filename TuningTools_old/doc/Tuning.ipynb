{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the Discriminator\n",
    "\n",
    "*This documentation is still being written.*\n",
    "\n",
    "It is possible to tune the Ringer discriminator both on standalone and on the GRID. The latter only applies if you have installed the TuningTools with cvmfs access.\n",
    "\n",
    "In order to run a standalone tuning, you can both run: \n",
    "\n",
    "- [Recommended] [use the executable](#Using-the-tuning-shell-command); or\n",
    "- a [python script](#Running-through-python-script). \n",
    "\n",
    "If it is wanted to send the job to the GRID, the only option is to run the [executable command to upload the job](#Running-the-GRID-dispatch-tuning-command)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning on the GRID\n",
    "\n",
    "Running the tuning job on the GRID require more steps than doing it on standalone. It isn't possible to configure each one of the jobs via job arguments, so it is needed to create configuration files for each the GRID jobs. This allows the panda job to divide the job into subsets.\n",
    "\n",
    "In order to do so, we will first have to [create the configuration data](#Creating-configuration-data) and afterwards export it to be [available on the GRID](#Exporting-data-to-the-GRID). Only after these steps, it will be possible to [dispach the job to the GRID](#Dispatching-the-job-to-the-GRID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating configuration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration data is used by the panda pilot to divide the job into subsets. The user needs to specify the discriminator parameter range (i.e.: number of neurons in the hidden layer, in the case of neural networks), the number of initializations and the number of cross-validation sorts, and how many of them will be run in a job on the GRID. Besides that, the tuning process need to specify a unique CrossValidation object to be used by all jobs, being this also informed through a configuration file. Finally, the pre-processing is currently also informed via a configuration file.\n",
    "\n",
    "In order to generate all this information, the user can make use of `createTuningJobFiles.py` executable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: createTuningJobFiles.py [-h] [--compress [COMPRESS]] [-outJobConfig JOBCONFIFILESOUTPUTFOLDER] [--neuronBounds NEURONBOUNDS [NEURONBOUNDS ...]] [--sortBounds SORTBOUNDS [SORTBOUNDS ...]]\n",
      "                               [--nInits [NINITS]] [--nNeuronsPerJob NNEURONSPERJOB] [--nSortsPerJob NSORTSPERJOB] [--nInitsPerJob NINITSPERJOB] [-outCross CROSSVALIDOUTPUTFILE] [-ns NSORTS]\n",
      "                               [-nb NBOXES] [-ntr NTRAIN] [-nval NVALID] [-ntst NTEST] [-seed SEED] [-outPP PREPROCOUTPUTFILE] [-ppCol PPCOL] [--output-level {DEBUG,ERROR,FATAL,INFO,VERBOSE,WARNING}]\n",
      "                               {ConfigFiles,CrossValidFile,_ignoreCase,all,ppFile} [{ConfigFiles,CrossValidFile,_ignoreCase,all,ppFile} ...]\n",
      "\n",
      "Generate input file for TuningTool on GRID\n",
      "\n",
      "positional arguments:\n",
      "  {ConfigFiles,CrossValidFile,_ignoreCase,all,ppFile}\n",
      "                        Which kind of files to create. You can choose one or more of the available choices, just don't use all with the other available choices.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --compress [COMPRESS]\n",
      "                        Whether to compress files or not.\n",
      "\n",
      "JobConfig Files Creation Options:\n",
      "  Change configuration for job config files creation.\n",
      "\n",
      "  -outJobConfig JOBCONFIFILESOUTPUTFOLDER, --jobConfiFilesOutputFolder JOBCONFIFILESOUTPUTFOLDER\n",
      "                        The job config files output folder.\n",
      "  --neuronBounds NEURONBOUNDS [NEURONBOUNDS ...]\n",
      "                        Input a sequential bounded list to be used as the neuron job range, the arguments should have the same format from the seq unix command or as the Matlab format. If not specified,\n",
      "                        the range will start from 1. I.e 5 2 9 leads to [5 7 9] and 50 leads to 1:50\n",
      "  --sortBounds SORTBOUNDS [SORTBOUNDS ...]\n",
      "                        Input a sequential bounded list using seq format to be used as the sort job range, but the last bound will be opened just as happens when using python range function. If not\n",
      "                        specified, the range will start from 0. I.e. 5 2 9 leads to [5 7] and 50 leads to range(50)\n",
      "  --nInits [NINITS]     The number of initilizations of the discriminator.\n",
      "  --nNeuronsPerJob NNEURONSPERJOB\n",
      "                        The number of hidden layer neurons per job.\n",
      "  --nSortsPerJob NSORTSPERJOB\n",
      "                        The number of sorts per job.\n",
      "  --nInitsPerJob NINITSPERJOB\n",
      "                        The number of initializations per job.\n",
      "\n",
      "CrossValid File Creation Options:\n",
      "  Change configuration for CrossValid file creation.\n",
      "\n",
      "  -outCross CROSSVALIDOUTPUTFILE, --crossValidOutputFile CROSSVALIDOUTPUTFILE\n",
      "                        The cross validation output file.\n",
      "  -ns NSORTS, --nSorts NSORTS\n",
      "                        The number of sort used by cross validation configuration.\n",
      "  -nb NBOXES, --nBoxes NBOXES\n",
      "                        The number of boxes used by cross validation configuration.\n",
      "  -ntr NTRAIN, --nTrain NTRAIN\n",
      "                        The number of train boxes used by cross validation.\n",
      "  -nval NVALID, --nValid NVALID\n",
      "                        The number of valid boxes used by cross validation.\n",
      "  -ntst NTEST, --nTest NTEST\n",
      "                        The number of test boxes used by cross validation.\n",
      "  -seed SEED            The seed value for generating CrossValid object.\n",
      "\n",
      "PreProc File Creation Options:\n",
      "  Change configuration for pre-processing file creation. These options will only be taken into account if job fileType is set to \"ppFile\" or \"all\".\n",
      "\n",
      "  -outPP PREPROCOUTPUTFILE, --preProcOutputFile PREPROCOUTPUTFILE\n",
      "                        The pre-processing validation output file\n",
      "  -ppCol PPCOL          The pre-processing collection to apply. The string will be parsed by python and created using the available pre-processings on TuningTools.PreProc.py file\n",
      "\n",
      "Loggging arguments:\n",
      "\n",
      "  --output-level {DEBUG,ERROR,FATAL,INFO,VERBOSE,WARNING}\n",
      "                        The output level for the main logger\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "createTuningJobFiles.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If more information is needed, check the [available example](#Example-1:-Creating-configuration-files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data to the GRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the files, it is needed to upload them to the GRID so that the panda pilot and the sub-jobs can retrieve their information. The `add_container.sh` helps on this task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: add_container.sh [-hv[verbosity_level=1]] \n",
      "                -f|--file[=] INPUTFILE | -f|--file INPUTFILES\n",
      "                --dataset[=] DATASET\n",
      "                [--rse[=] RSE 'CERN-PROD_SCRATCHDISK']\n",
      "                [--useDQ2=1]\n",
      "\n",
      "Create dataset on grid containing input local files at specified rse.\n",
      "IMPORTANT: You need to have grid environment set.\n",
      "\n",
      "    -h             display this help and exit\n",
      "    -f INPUTFILES  files to upload to grid container. If one file is a directory,\n",
      "                   it will be expanded using all non diretory files inside it.\n",
      "    -v             verbose mode. Can be used multiple times for increased\n",
      "                   verbosity.\n",
      "    --dataset      the dataset name. It must be specified using\n",
      "                   user.account.datasetname.\n",
      "    --rse          The rse to upload the files and put the dataset\n",
      "    --useDQ2       If set to true, then DQ2 will be used instead of rucio.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "add_container.sh -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An usage example:\n",
    "\n",
    "```\n",
    "add_container.sh --file jobConfig --dataset config.nn5to7_sorts50_1by1_inits100_50by50 --rse BNL-OSG2_SCRATCHDISK\n",
    "```\n",
    "\n",
    "There is a documentation on the rucio usage aiming on the commands needed by the TuningTools [available here](http://nbviewer.jupyter.org/gist/wsfreund/249b5db998fb4594f800)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispatching the job to the GRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done via the command `runGRIDtuning.py`. The basic arguments needed to be informed are: \n",
    "- `--dataDS`: Inform the uploaded container with the data file;\n",
    "- `--crossValidDS`: Container with the CrossValidation file;\n",
    "- `--configFileDS`: Container used by the pilot to divide the jobs and contains the job configuration files;\n",
    "- `--ppDS`: Container with the pre-processing chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: runGRIDtuning.py [-h] [--show-evo SHOW_EVO] [--max-fail MAX_FAIL] [--epochs EPOCHS] [--do-perf DO_PERF] [--batch-size BATCH_SIZE] [--algorithm-name ALGORITHM_NAME] [--network-arch NETWORK_ARCH]\r\n",
      "                        [--cost-function COST_FUNCTION] [--shuffle SHUFFLE] [--seed SEED] [--do-multi-stop DO_MULTI_STOP] -d DATA [DATA ...] -c Config_DS [Config_DS ...] -pp PP_DS [PP_DS ...] -x\r\n",
      "                        CrossValid_DS [CrossValid_DS ...] [--et-bins ET_BINS [ET_BINS ...]] [--eta-bins ETA_BINS [ETA_BINS ...]] [--secondaryDSs GRID_SECONDARYDS [GRID_SECONDARYDS ...]] --outDS\r\n",
      "                        GRID_OUTDS [--site [GRID_SITE]] [--excludedSite [GRID_EXCLUDEDSITE]] [--debug] [--excludeFile [GRID_EXCLUDEFILE]] [--disableAutoRetry] [--extFile [GRID_EXTFILE]]\r\n",
      "                        [--maxNFilesPerJob [GRID_MAXNFILESPERJOB]] [--cloud [GRID_CLOUD]] [--nGBPerJob [GRID_NGBPERJOB]] [--skipScout] [--memory GRID_MEMORY] [--useNewCode] [--dry-run]\r\n",
      "                        [--allowTaskDuplication] [-itar [InTarBall]] [-otar [OutTarBall]] [--output-level {DEBUG,ERROR,FATAL,INFO,VERBOSE,WARNING}]\r\n",
      "\r\n",
      "Tune discriminators using input data on the GRID\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "\r\n",
      "Required arguments:\r\n",
      "\r\n",
      "  -d DATA [DATA ...], --dataDS DATA [DATA ...]\r\n",
      "                        The dataset with the data for discriminator tuning.\r\n",
      "\r\n",
      "Optional arguments:\r\n",
      "\r\n",
      "Cross-validation configuration:\r\n",
      "\r\n",
      "  -x CrossValid_DS [CrossValid_DS ...], --crossValidDS CrossValid_DS [CrossValid_DS ...]\r\n",
      "                        The cross-validation files container.\r\n",
      "\r\n",
      "Looping configuration:\r\n",
      "\r\n",
      "  -c Config_DS [Config_DS ...], --configFileDS Config_DS [Config_DS ...]\r\n",
      "                        Input dataset to loop upon files to retrieve configuration. There will be one job for each file on this container.\r\n",
      "\r\n",
      "Pre-processing configuration:\r\n",
      "\r\n",
      "  -pp PP_DS [PP_DS ...], --ppFileDS PP_DS [PP_DS ...]\r\n",
      "                        The pre-processing files container.\r\n",
      "\r\n",
      "Binning configuration:\r\n",
      "\r\n",
      "  --et-bins ET_BINS [ET_BINS ...]\r\n",
      "                        The et bins to use within this job. When not specified, all bins available on the file will be tuned in a single job in the GRID, otherwise each bin is available is submited\r\n",
      "                        separately. If specified as a integer or float, it is assumed that the user wants to run a single job using only for the specified bin index. In case a list is specified, it is\r\n",
      "                        transformed into a MatlabLoopingBounds, read its documentation on: http://nbviewer.jupyter.org/github/wsfreund/RingerCore/blob/master/readme.ipynb#LoopingBounds for more details.\r\n",
      "  --eta-bins ETA_BINS [ETA_BINS ...]\r\n",
      "                        The eta bins to use within grid job. Check et-bins help for more information.\r\n",
      "\r\n",
      "Tuning CORE configuration:\r\n",
      "\r\n",
      "  --show-evo SHOW_EVO   The number of iterations where performance is shown.\r\n",
      "  --max-fail MAX_FAIL   Maximum number of failures to imrpove performance over validation dataset that is tolerated.\r\n",
      "  --epochs EPOCHS       Number of iterations where the tuning algorithm can run the optimization.\r\n",
      "  --do-perf DO_PERF     Whether we should run performance testing under convergence conditions, using test/validation dataset and also estimate operation condition.\r\n",
      "  --batch-size BATCH_SIZE\r\n",
      "                        Set the batch size used during tuning.\r\n",
      "\r\n",
      "ExMachina CORE configuration:\r\n",
      "\r\n",
      "  --algorithm-name ALGORITHM_NAME\r\n",
      "                        The tuning method to use.\r\n",
      "  --network-arch NETWORK_ARCH\r\n",
      "                        The neural network architeture to use.\r\n",
      "  --cost-function COST_FUNCTION\r\n",
      "                        The cost function used by ExMachina.\r\n",
      "  --shuffle SHUFFLE     Whether to shuffle datasets while training.\r\n",
      "\r\n",
      "FastNet CORE configuration:\r\n",
      "\r\n",
      "  --seed SEED           The seed to be used by the tuning algorithm.\r\n",
      "  --do-multi-stop DO_MULTI_STOP\r\n",
      "                        Tune classifier using P_D, P_F and SP when set to True. Uses only SP when set to False.\r\n",
      "\r\n",
      "GRID Input Dataset Arguments:\r\n",
      "\r\n",
      "  --secondaryDSs GRID_SECONDARYDS [GRID_SECONDARYDS ...]\r\n",
      "                        The secondary Dataset ID (DID), in the format name:nEvents:place\r\n",
      "\r\n",
      "GRID Output Dataset Arguments:\r\n",
      "\r\n",
      "  --outDS GRID_OUTDS, -o GRID_OUTDS\r\n",
      "                        The output Dataset ID (DID)\r\n",
      "\r\n",
      "GRID Arguments:\r\n",
      "\r\n",
      "  --site [GRID_SITE]    The site location where the job should run.\r\n",
      "  --excludedSite [GRID_EXCLUDEDSITE]\r\n",
      "                        The excluded site location.\r\n",
      "  --debug               Submit GRID job on debug mode.\r\n",
      "  --excludeFile [GRID_EXCLUDEFILE]\r\n",
      "                        Files to exclude from environment copied to grid.\r\n",
      "  --disableAutoRetry    Flag to disable auto retrying jobs.\r\n",
      "  --extFile [GRID_EXTFILE]\r\n",
      "                        External file to add.\r\n",
      "  --maxNFilesPerJob [GRID_MAXNFILESPERJOB]\r\n",
      "                        Maximum number of files per job.\r\n",
      "  --cloud [GRID_CLOUD]  The cloud where to submit the job.\r\n",
      "  --nGBPerJob [GRID_NGBPERJOB]\r\n",
      "                        Maximum number of GB per job.\r\n",
      "  --skipScout           Flag to disable auto retrying jobs.\r\n",
      "  --memory GRID_MEMORY  Needed memory to run in MB.\r\n",
      "  --useNewCode          Flag to disable auto retrying jobs.\r\n",
      "  --dry-run             Only print grid resulting command, but do not execute it. Used for debugging submission.\r\n",
      "  --allowTaskDuplication\r\n",
      "                        Flag to disable auto retrying jobs.\r\n",
      "  -itar [InTarBall], --inTarBall [InTarBall]\r\n",
      "                        The environemnt tarball for posterior usage.\r\n",
      "  -otar [OutTarBall], --outTarBall [OutTarBall]\r\n",
      "                        The environemnt tarball for posterior usage.\r\n",
      "\r\n",
      "Loggging arguments:\r\n",
      "\r\n",
      "  --output-level {DEBUG,ERROR,FATAL,INFO,VERBOSE,WARNING}\r\n",
      "                        The output level for the main logger\r\n"
     ]
    }
   ],
   "source": [
    "!runGRIDtuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example is available [here](#Example-3:-Dispatching-job-to-the-GRID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning on Standalone\n",
    "\n",
    "Most of the TuningTools functionalities can be accessed through python scripts or a shell command. The executable is the recommended way for running a standalone job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the tuning shell command\n",
    "\n",
    "This is the recommended way for interacting with the tunning job. Use the command `runTuning.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: runTuning.py -d data [-x CROSSFILE] [-c CONFFILELIST [CONFFILELIST ...]] [--neuronBounds NEURONBOUNDS [NEURONBOUNDS ...]] [--sortBounds SORTBOUNDS [SORTBOUNDS ...]]\n",
      "                    [--initBounds INITBOUNDS [INITBOUNDS ...]] [--ppFileList PPFILELIST [PPFILELIST ...]] [--et-bins ET_BINS [ET_BINS ...]] [--eta-bins ETA_BINS [ETA_BINS ...]] [--no-compress]\n",
      "                    [--show-evo SHOW_EVO] [--max-fail MAX_FAIL] [--epochs EPOCHS] [--do-perf DO_PERF] [--batch-size BATCH_SIZE] [--algorithm-name ALGORITHM_NAME] [--network-arch NETWORK_ARCH]\n",
      "                    [--cost-function COST_FUNCTION] [--shuffle SHUFFLE] [--seed SEED] [--do-multi-stop DO_MULTI_STOP] [--output-level {DEBUG,ERROR,FATAL,INFO,VERBOSE,WARNING}]\n",
      "\n",
      "Tune discriminators using input data.\n",
      "\n",
      "Required arguments:\n",
      "\n",
      "  -d data, --data data  The data file that will be used to tune the discriminators\n",
      "\n",
      "Optional arguments:\n",
      "\n",
      "  --no-compress         Don't compress output files.\n",
      "\n",
      "Cross-validation configuration:\n",
      "\n",
      "  -x CROSSFILE, --crossFile CROSSFILE\n",
      "                        The cross-validation file path, pointing to a file created with the create tuning job files\n",
      "\n",
      "Looping configuration:\n",
      "\n",
      "  -c CONFFILELIST [CONFFILELIST ...], --confFileList CONFFILELIST [CONFFILELIST ...]\n",
      "                        A python list or a comma separated list of the root files containing the configuration to run the jobs. The files can be generated using a CreateConfFiles instance which can be\n",
      "                        accessed via command line using the createTuningJobFiles.py script.\n",
      "  --neuronBounds NEURONBOUNDS [NEURONBOUNDS ...]\n",
      "                        Input a sequential bounded list to be used as the neuron job range, the arguments should have the same format from the seq unix command or as the Matlab format. If not specified,\n",
      "                        the range will start from 1. I.e 5 2 9 leads to [5 7 9] and 50 leads to 1:50\n",
      "  --sortBounds SORTBOUNDS [SORTBOUNDS ...]\n",
      "                        Input a sequential bounded list using seq format to be used as the sort job range, but the last bound will be opened just as happens when using python range function. If not\n",
      "                        specified, the range will start from 0. I.e. 5 2 9 leads to [5 7] and 50 leads to range(50)\n",
      "  --initBounds INITBOUNDS [INITBOUNDS ...]\n",
      "                        Input a sequential bounded list using seq format to be used as the inits job range, but the last bound will be opened just as happens when using python range function. If not\n",
      "                        specified, the range will start from 0. I.e. 5 2 9 leads to [5 7] and 50 leads to range(50)\n",
      "\n",
      "Pre-processing configuration:\n",
      "\n",
      "  --ppFileList PPFILELIST [PPFILELIST ...]\n",
      "                        A list or a comma separated list of the file paths containing the pre-processing chain to apply in the input space and obtain the pattern space. The files can be generated using\n",
      "                        a CreateConfFiles instance which is accessed via command line using the createTuningJobFiles.py script. The ppFileList must have a file for each of the configuration list\n",
      "                        defined, that is, one pre-processing chain for each one of the neuron/sort/init bounds collection. When only one ppFile is defined and the configuration list has size greater\n",
      "                        than one, the pre-processing chain will be copied for being applied on the other bounds.\n",
      "\n",
      "Binning configuration:\n",
      "\n",
      "  --et-bins ET_BINS [ET_BINS ...]\n",
      "                        The et bins to use within this job. When not specified, all bins available on the file will be tuned separately. If specified as a integer or float, it is assumed that the user\n",
      "                        wants to run the job only for the specified bin index. In case a list is specified, it is transformed into a MatlabLoopingBounds, read its documentation on:\n",
      "                        http://nbviewer.jupyter.org/github/wsfreund/RingerCore/blob/master/readme.ipynb#LoopingBounds for more details.\n",
      "  --eta-bins ETA_BINS [ETA_BINS ...]\n",
      "                        The eta bins to use within this job. Check et-bins help for more information.\n",
      "\n",
      "Tuning CORE configuration:\n",
      "\n",
      "  --show-evo SHOW_EVO   The number of iterations where performance is shown.\n",
      "  --max-fail MAX_FAIL   Maximum number of failures to imrpove performance over validation dataset that is tolerated.\n",
      "  --epochs EPOCHS       Number of iterations where the tuning algorithm can run the optimization.\n",
      "  --do-perf DO_PERF     Whether we should run performance testing under convergence conditions, using test/validation dataset and also estimate operation condition.\n",
      "  --batch-size BATCH_SIZE\n",
      "                        Set the batch size used during tuning.\n",
      "\n",
      "ExMachina CORE configuration:\n",
      "\n",
      "  --algorithm-name ALGORITHM_NAME\n",
      "                        The tuning method to use.\n",
      "  --network-arch NETWORK_ARCH\n",
      "                        The neural network architeture to use.\n",
      "  --cost-function COST_FUNCTION\n",
      "                        The cost function used by ExMachina.\n",
      "  --shuffle SHUFFLE     Whether to shuffle datasets while training.\n",
      "\n",
      "FastNet CORE configuration:\n",
      "\n",
      "  --seed SEED           The seed to be used by the tuning algorithm.\n",
      "  --do-multi-stop DO_MULTI_STOP\n",
      "                        Tune classifier using P_D, P_F and SP when set to True. Uses only SP when set to False.\n",
      "\n",
      "Loggging arguments:\n",
      "\n",
      "  --output-level {DEBUG,ERROR,FATAL,INFO,VERBOSE,WARNING}\n",
      "                        The output level for the main logger\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "runTuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example is available [here](#Example-2:-Standalone-job-via-executable-command)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running through python script\n",
    "\n",
    "However, you can directly access the `TuningJob` class, and call it using a python script. The __call__ method documentation cover all available options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method __call__ in module TuningTools.TuningJob:\n",
      "\n",
      "__call__(self, dataLocation, **kw) unbound TuningTools.TuningJob.TuningJob method\n",
      "    Run discrimination tuning for input data created via CreateData.py\n",
      "    Arguments:\n",
      "      - dataLocation: A string containing a path to the data file written\n",
      "        by CreateData.py\n",
      "    Mutually exclusive optional arguments: Either choose the cross (x) or\n",
      "      circle (o) of the following block options.\n",
      "     -------\n",
      "      x crossValid [CrossValid( nSorts=50, nBoxes=10, nTrain=6, nValid=4, \n",
      "                                seed=crossValidSeed )]:\n",
      "        The cross-validation sorts object. The files can be generated using a\n",
      "        CreateConfFiles instance which can be accessed via command line using\n",
      "        the createTuningJobFiles.py script.\n",
      "      x crossValidSeed [None]: Only used when not specifying the crossValid option.\n",
      "        The seed is used by the cross validation random sort generator and\n",
      "        when not specified or specified as None, time is used as seed.\n",
      "      o crossValidFile: The cross-validation file path, pointing to a file\n",
      "        created with the create tuning job files\n",
      "     -------\n",
      "      x confFileList [None]: A python list or a comma separated list of the\n",
      "        root files containing the configuration to run the jobs. The files can\n",
      "        be generated using a CreateConfFiles instance which can be accessed via\n",
      "        command line using the createTuningJobFiles.py script.\n",
      "      o neuronBoundsCol [MatlabLoopingBounds(5,5)]: A LoopingBoundsCollection\n",
      "        range where the the neural network should loop upon.\n",
      "      o sortBoundsCol [PythonLoopingBounds(50)]: A LoopingBoundsCollection\n",
      "        range for the sorts to use from the crossValid object.\n",
      "      o initBoundsCol [PythonLoopingBounds(100)]: A LoopingBoundsCollection\n",
      "        range for the initialization numbers to be ran on this tuning job.\n",
      "        The neuronBoundsCol, sortBoundsCol, initBoundsCol will be synchronously\n",
      "        looped upon, that is, all the first collection information upon those \n",
      "        variables will be used to feed the first job configuration, all of \n",
      "        those second collection information will be used to feed the second job \n",
      "        configuration, and so on...\n",
      "        In the case you have only one job configuration, it can be input as\n",
      "        a single LoopingBounds instance, or values that feed the LoopingBounds\n",
      "        initialization. In the last case, the neuronBoundsCol will be used\n",
      "        to feed a MatlabLoopingBounds, and the sortBoundsCol together with\n",
      "        initBoundsCol will be used to feed a PythonLoopingBounds.\n",
      "        For instance, if you use neuronBoundsCol set to [5,2,11], it will \n",
      "        loop upon the list [5,7,9,11], while if this was set to sortBoundsCol,\n",
      "        it would generate [5,7,9].\n",
      "     -------\n",
      "      x ppFileList [None]: A python list or a comma separated list of the\n",
      "        root files containing the pre-processing chain to apply into \n",
      "        input space and obtain the pattern space. The files can be generated\n",
      "        using a CreateConfFiles instance which is accessed via command\n",
      "        line using the createTuningJobFiles.py script.\n",
      "        The ppFileList must have a file for each of the configuration list \n",
      "        defined, that is, one pre-processing chain for each one of the \n",
      "        neuron/sort/init bounds collection. When only one ppFile is defined and\n",
      "        the configuration list has size greater than one, the pre-processing\n",
      "        chain will be copied for being applied on the other bounds.\n",
      "      o ppCol [PreProcChain( Norm1() )]: A PreProcCollection with the\n",
      "        PreProcChain instances to be applied to each of the configuration\n",
      "        ranges chosen by the above configurations.\n",
      "        The ppCol must have a file for each of the configuration list \n",
      "        defined, that is, one pre-processing chain for each one of the \n",
      "        neuron/sort/init bounds collection. When only one ppFile is defined\n",
      "        and the configuration list has size greater than one, the\n",
      "        pre-processing chain will be copied for being applied on the other\n",
      "        bounds.\n",
      "     -------\n",
      "    Optional arguments:\n",
      "      - operationLevel [None]: The discriminator operation level. When set to\n",
      "          None, the operation level will be retrieved from the tuning data\n",
      "          file. For now, this is only used to set the default operation targets\n",
      "          on Loose and Tight tunings.\n",
      "      - etBins [None]: The et bins to use within this job. When not specified,\n",
      "        all bins available on the file will be tuned separately.\n",
      "      - etaBins [None]: The eta bins to use within this job. When not specified,\n",
      "        all bins available on the file will be tuned separately.\n",
      "      - tuneOperationTargets [['Loose', 'Pd' , #looseBenchmarkRef],\n",
      "                              ['Medium', 'SP'],\n",
      "                              ['Tight', 'Pf' , #tightBenchmarkRef]]\n",
      "          The tune operation targets which should be used for this tuning\n",
      "          job. The strings inputs must be part of the ReferenceBenchmark\n",
      "          enumeration.\n",
      "          Instead of an enumeration string (or the enumeration itself),\n",
      "          you can set it directly to a value, e.g.: \n",
      "            [['Loose97', 'Pd', .97,],['Tight005','Pf',.005]]\n",
      "          This can also be set using a string, e.g.:\n",
      "            [['Loose97','Pd' : '.97'],['Tight005','Pf','.005']]\n",
      "          , which may contain a percentage symbol:\n",
      "            [['Loose97','Pd' : '97%'],['Tight005','Pf','0.5%']]\n",
      "          When set to None, the Pd and Pf will be set to the value of the\n",
      "          benchmark correspondent to the operation level set.\n",
      "      - compress [True]: Whether to compress file or not.\n",
      "      - level [loggingLevel.INFO]: The logging output level.\n",
      "      - outputFileBase ['nn.tuned']: The tuning outputFile starting string.\n",
      "          It will also contain a custom string representing the configuration\n",
      "          used to tune the discriminator.\n",
      "      - showEvo (TuningWrapper prop) [50]: The number of iterations wher\n",
      "          performance is shown (used as a boolean on ExMachina).\n",
      "      - maxFail (TuningWrapper prop) [50]: Maximum number of failures\n",
      "          tolerated failing to improve performance over validation dataset.\n",
      "      - epochs (TuningWrapper prop) [1000]: Number of iterations where\n",
      "          the tuning algorithm can run the optimization.\n",
      "      - doPerf (TuningWrapper prop) [True]: Whether we should run performance\n",
      "          testing under convergence conditions, using test/validation dataset\n",
      "          and also estimate operation condition.\n",
      "      - maxFail (TuningWrapper prop) [50]: Number of epochs which failed to improve\n",
      "          validation efficiency. When reached, the tuning process is stopped.\n",
      "      - batchSize (TuningWrapper prop) [number of observations of the class\n",
      "          with the less observations]: Set the batch size used during tuning.\n",
      "      - algorithmName (TuningWrapper prop) [resilient back-propgation]: The\n",
      "          tuning method to use.\n",
      "      - networkArch (ExMachina prop) ['feedforward']: the neural network\n",
      "          architeture to use.\n",
      "      - costFunction (ExMachina prop) ['sp']: the cost function used by ExMachina\n",
      "      - shuffle (ExMachina prop) [True]: Whether to shuffle datasets while\n",
      "        training.\n",
      "      - seed (FastNet prop) [None]: The seed to be used by the tuning\n",
      "          algorithm.\n",
      "      - doMultiStop (FastNet prop) [True]: Tune classifier using P_D, P_F and\n",
      "        SP when set to True. Uses only SP when set to False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from TuningTools.TuningJob import TuningJob\n",
    "help(TuningJob.__call__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See an example [here](#Example-4:-Standalone-job-via-python-script)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The information available on the TunedDiscrArchieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Still to be written*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Creating configuration files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that it is needed to tune neural networks with number of hidden layer neurons  $N_H\\in\\{5,7\\}$, a total of 10 sorts on 10 data subsets with 6 of them being reserved for training and 4 for validation. The sorts will have the pseudo-number generator seeded by the integer 10. It is wanted to tune a total of 100 initializations to avoid local optima. The sub-jobs should run 1 hidden neuron configuration, only one data sort and 50 initializations. Finally, the pre-processing chain will use the MapStd normalization.\n",
    "\n",
    "This can be achieved by using the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Py.__main__                             INFO Creating configuration files at folder jobConfig\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0000.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0000.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0001.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0001.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0002.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0002.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0003.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0003.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0004.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0004.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0005.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0005.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0006.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0006.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0007.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0007.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0008.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0008.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0009.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0005.s0009.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0000.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0000.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0001.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0001.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0002.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0002.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0003.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0003.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0004.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0004.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0005.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0005.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0006.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0006.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0007.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0007.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0008.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0008.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0009.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0006.s0009.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0000.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0000.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0001.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0001.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0002.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0002.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0003.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0003.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0004.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0004.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0005.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0005.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0006.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0006.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0007.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0007.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0008.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0008.il0050.iu0099.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0009.il0000.iu0049.pic.gz\n",
      "Py.CreateTuningJobFiles                 INFO Saved job option configuration at path: jobConfig/job.hn0007.s0009.il0050.iu0099.pic.gz\n",
      "Py.__main__                             INFO Created cross-validation file at path crossValid.pic.gz\n",
      "Py.__main__                             INFO Created pre-processing file at path ppFile_MapStd.pic.gz\n",
      "Py.__main__                             INFO Finished creating tuning job files.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "createTuningJobFiles.py all \\\n",
    "                        --neuronBounds 5 7 \\\n",
    "                        --sortBounds 10 \\\n",
    "                        --nInits 100 \\\n",
    "                        --nNeuronsPerJob 1 \\\n",
    "                        --nSortsPerJob 1 \\\n",
    "                        --nInitsPerJob 50 \\\n",
    "                        -ns 50 \\\n",
    "                        -nb 10 \\\n",
    "                        -ntr 6 \\\n",
    "                        -nval 4 \\\n",
    "                        -seed 10 \\\n",
    "                        -ppCol \"[[MapStd()]]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created the `jobConfig` folder on the current path, which contains the sub-jobs parameters, the `crossValid.pic.gz` file containing the CrossValidation sorts, and the `ppFile_*.pic.gz`, with the pre-processing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossValid.pic.gz\n",
      "ppFile_MapStd.pic.gz\n",
      "\n",
      "jobConfig:\n",
      "job.hn0005.s0000.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0000.il0050.iu0099.pic.gz\n",
      "job.hn0005.s0001.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0001.il0050.iu0099.pic.gz\n",
      "job.hn0005.s0002.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0002.il0050.iu0099.pic.gz\n",
      "job.hn0005.s0003.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0003.il0050.iu0099.pic.gz\n",
      "job.hn0005.s0004.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0004.il0050.iu0099.pic.gz\n",
      "job.hn0005.s0005.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0005.il0050.iu0099.pic.gz\n",
      "job.hn0005.s0006.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0006.il0050.iu0099.pic.gz\n",
      "job.hn0005.s0007.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0007.il0050.iu0099.pic.gz\n",
      "job.hn0005.s0008.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0008.il0050.iu0099.pic.gz\n",
      "job.hn0005.s0009.il0000.iu0049.pic.gz\n",
      "job.hn0005.s0009.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0000.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0000.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0001.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0001.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0002.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0002.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0003.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0003.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0004.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0004.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0005.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0005.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0006.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0006.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0007.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0007.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0008.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0008.il0050.iu0099.pic.gz\n",
      "job.hn0006.s0009.il0000.iu0049.pic.gz\n",
      "job.hn0006.s0009.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0000.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0000.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0001.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0001.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0002.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0002.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0003.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0003.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0004.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0004.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0005.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0005.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0006.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0006.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0007.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0007.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0008.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0008.il0050.iu0099.pic.gz\n",
      "job.hn0007.s0009.il0000.iu0049.pic.gz\n",
      "job.hn0007.s0009.il0050.iu0099.pic.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls jobConfig crossValid.pic* ppFile*.pic*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Standalone job via executable command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we want to tune the data created [here](http://nbviewer.jupyter.org/github/wsfreund/TuningTools/blob/master/doc/CreateData.ipynb#Example-1:-Creating-data-using-the-command-line) using 16 hidden layer neurons, 4 initializations and the first sort. We also and to tune in this job the $E_T$ bins from 1 to 2 and the $\\eta$ bins from 1 to 3. The command would be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TuningToolPyWrapper                     INFO Changing pseudo-random number generator seed to (1457565886).\n",
      "Py.TuningJob                            INFO Opening data (etBin=1,etaBin=1) ...\n",
      "Py.TuningJob                            INFO Tunning Et bin: array([ 30.,  50.], dtype=float32)\n",
      "Py.TuningJob                            INFO Tunning eta bin: array([ 0.80000001,  1.37      ], dtype=float32)\n",
      "Py.TuningJob                            INFO Running configuration file number 0 (etBin=1,etaBin=1) \n",
      "Py.TuningJob                            INFO Extracting cross validation sort 0 (etBin=1,etaBin=1) \n",
      "Py.CrossValid                           INFO Train      #Events/class: [926, 505]\n",
      "Py.CrossValid                           INFO Validation #Events/class: [617, 336]\n",
      "Py.TuningJob                            INFO Tuning pre-processing chain (Norm1)...\n",
      "Py.TuningJob                            INFO Applying pre-processing chain...\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 0> (etBin=1,etaBin=1) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.01239 SP (val) = 0.920289 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.0890014 SP (val) = 0.956499 stops = 0\n",
      "PatternRecognition                      INFO Epoch 100: mse (train) = 0.0915176 SP (val) = 0.955591 stops = 3\n",
      "PatternRecognition                      INFO Epoch 131: mse (train) = 0.0865732 SP (val) = 0.954787 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.965535, det = 0.989631, fa = 0.058264, cut = -0.200001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.960507, det = 0.983793, fa = 0.062500, cut = -0.070001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.960109, det = 0.989631, fa = 0.068965, cut = -0.315001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.957018, det = 0.995138, fa = 0.080357, cut = -0.475001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.969208, det = 0.982502, fa = 0.043995, cut = -0.115001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.957895, det = 0.972447, fa = 0.056548, cut = -0.060001\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 1> (etBin=1,etaBin=1) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.05992 SP (val) = 0.912854 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.092522 SP (val) = 0.953758 stops = 1\n",
      "PatternRecognition                      INFO Epoch 75: mse (train) = 0.0963615 SP (val) = 0.952785 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.961280, det = 0.988334, fa = 0.065398, cut = -0.200001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.959090, det = 0.987034, fa = 0.068452, cut = -0.200001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.962207, det = 0.979261, fa = 0.054697, cut = -0.010001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.956865, det = 0.988655, fa = 0.074405, cut = -0.270001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.960321, det = 0.986390, fa = 0.065398, cut = -0.220001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.958103, det = 0.978930, fa = 0.062500, cut = -0.170001\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 2> (etBin=1,etaBin=1) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.07567 SP (val) = 0.919183 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.101374 SP (val) = 0.958291 stops = 0\n",
      "PatternRecognition                      INFO Epoch 100: mse (train) = 0.0975558 SP (val) = 0.956172 stops = 3\n",
      "PatternRecognition                      INFO Epoch 131: mse (train) = 0.0952364 SP (val) = 0.954675 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.966777, det = 0.990927, fa = 0.057075, cut = -0.185001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.961399, det = 0.988655, fa = 0.065476, cut = -0.185001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.963613, det = 0.985742, fa = 0.058264, cut = -0.110001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.959256, det = 0.993517, fa = 0.074405, cut = -0.385001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.967340, det = 0.989631, fa = 0.054697, cut = -0.220001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.956172, det = 0.965964, fa = 0.053571, cut = 0.519999\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 3> (etBin=1,etaBin=1) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.01808 SP (val) = 0.910778 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.0912718 SP (val) = 0.953674 stops = 1\n",
      "PatternRecognition                      INFO Epoch 82: mse (train) = 0.0936762 SP (val) = 0.950729 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.962522, det = 0.989631, fa = 0.064209, cut = -0.115001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.960687, det = 0.990276, fa = 0.068452, cut = -0.170001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.957683, det = 0.978613, fa = 0.063020, cut = 0.004999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.956223, det = 0.993517, fa = 0.080357, cut = -0.370001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.965457, det = 0.987038, fa = 0.055886, cut = -0.205001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.960507, det = 0.983793, fa = 0.062500, cut = -0.205001\n",
      "Py.TuningJob                            INFO Saving file named nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0001.eta0001.pic...\n",
      "Py.TuningJob                            INFO File \"nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0001.eta0001.pic.gz\" saved!\n",
      "Py.TuningJob                            INFO Finished tuning job!\n",
      "Py.TuningJob                            INFO Opening data (etBin=1,etaBin=2) ...\n",
      "Py.TuningJob                            INFO Tunning Et bin: array([ 30.,  50.], dtype=float32)\n",
      "Py.TuningJob                            INFO Tunning eta bin: array([ 1.37      ,  1.53999996], dtype=float32)\n",
      "Py.TuningJob                            INFO Running configuration file number 0 (etBin=1,etaBin=2) \n",
      "Py.TuningJob                            INFO Extracting cross validation sort 0 (etBin=1,etaBin=2) \n",
      "Py.CrossValid                           INFO Train      #Events/class: [216, 86]\n",
      "Py.CrossValid                           INFO Validation #Events/class: [143, 56]\n",
      "Py.TuningJob                            INFO Tuning pre-processing chain (Norm1)...\n",
      "Py.TuningJob                            INFO Applying pre-processing chain...\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 0> (etBin=1,etaBin=2) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.02471 SP (val) = 0.85345 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.124009 SP (val) = 0.889804 stops = 1\n",
      "PatternRecognition                      INFO Epoch 79: mse (train) = 0.117056 SP (val) = 0.885024 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.906191, det = 0.896936, fa = 0.084507, cut = 0.324999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.909918, det = 0.874126, fa = 0.053571, cut = 0.324999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.935075, det = 0.969359, fa = 0.098592, cut = -0.010001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.896658, det = 0.937063, fa = 0.142857, cut = -0.010001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.904740, det = 0.908078, fa = 0.098592, cut = 0.214999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.907866, det = 0.853147, fa = 0.035714, cut = 0.374999\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 1> (etBin=1,etaBin=2) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.02359 SP (val) = 0.882926 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.109257 SP (val) = 0.88879 stops = 1\n",
      "PatternRecognition                      INFO Epoch 64: mse (train) = 0.117034 SP (val) = 0.883467 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.913894, det = 0.905292, fa = 0.077465, cut = 0.309999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.913482, det = 0.881119, fa = 0.053571, cut = 0.309999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.891360, det = 0.902507, fa = 0.119718, cut = 0.114999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.888497, det = 0.902098, fa = 0.125000, cut = 0.114999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.913894, det = 0.905292, fa = 0.077465, cut = 0.309999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.913482, det = 0.881119, fa = 0.053571, cut = 0.309999\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 2> (etBin=1,etaBin=2) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.04141 SP (val) = 0.894028 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.131756 SP (val) = 0.889804 stops = 1\n",
      "PatternRecognition                      INFO Epoch 70: mse (train) = 0.117278 SP (val) = 0.8909 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.932093, det = 0.927577, fa = 0.063380, cut = 0.469999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.915287, det = 0.902098, fa = 0.071429, cut = 0.469999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.885838, det = 0.863510, fa = 0.091549, cut = -0.045001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.897681, det = 0.958042, fa = 0.160714, cut = -0.070001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.901820, det = 0.916435, fa = 0.112676, cut = 0.179999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.900636, det = 0.839161, fa = 0.035714, cut = 0.384999\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 3> (etBin=1,etaBin=2) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.02652 SP (val) = 0.863828 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.141927 SP (val) = 0.879109 stops = 1\n",
      "PatternRecognition                      INFO Epoch 74: mse (train) = 0.116752 SP (val) = 0.882455 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.898432, det = 0.902507, fa = 0.105634, cut = -0.010001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.906401, det = 0.902098, fa = 0.089286, cut = -0.010001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.926094, det = 0.980501, fa = 0.126761, cut = -0.260001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.889130, det = 0.979021, fa = 0.196429, cut = -0.260001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.912488, det = 0.974930, fa = 0.147887, cut = -0.305001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.897589, det = 0.867133, fa = 0.071429, cut = 0.459999\n",
      "Py.TuningJob                            INFO Saving file named nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0001.eta0002.pic...\n",
      "Py.TuningJob                            INFO File \"nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0001.eta0002.pic.gz\" saved!\n",
      "Py.TuningJob                            INFO Finished tuning job!\n",
      "Py.TuningJob                            INFO Opening data (etBin=1,etaBin=3) ...\n",
      "Py.TuningJob                            INFO Tunning Et bin: array([ 30.,  50.], dtype=float32)\n",
      "Py.TuningJob                            INFO Tunning eta bin: array([ 1.53999996,  2.5       ], dtype=float32)\n",
      "Py.TuningJob                            INFO Running configuration file number 0 (etBin=1,etaBin=3) \n",
      "Py.TuningJob                            INFO Extracting cross validation sort 0 (etBin=1,etaBin=3) \n",
      "Py.CrossValid                           INFO Train      #Events/class: [1326, 454]\n",
      "Py.CrossValid                           INFO Validation #Events/class: [883, 303]\n",
      "Py.TuningJob                            INFO Tuning pre-processing chain (Norm1)...\n",
      "Py.TuningJob                            INFO Applying pre-processing chain...\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 0> (etBin=1,etaBin=3) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.01442 SP (val) = 0.897352 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.220943 SP (val) = 0.904988 stops = 1\n",
      "PatternRecognition                      INFO Epoch 62: mse (train) = 0.208235 SP (val) = 0.906147 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.907309, det = 0.937981, fa = 0.122853, cut = -0.000001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.906695, det = 0.967157, fa = 0.151815, cut = -0.080001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.913513, det = 0.953373, fa = 0.125495, cut = -0.005001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.904135, det = 0.979615, fa = 0.168317, cut = -0.250001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.901665, det = 0.942961, fa = 0.138705, cut = -0.110001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.903952, det = 0.961495, fa = 0.151815, cut = -0.125001\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 1> (etBin=1,etaBin=3) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.01794 SP (val) = 0.86469 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.210521 SP (val) = 0.909781 stops = 0\n",
      "PatternRecognition                      INFO Epoch 80: mse (train) = 0.204652 SP (val) = 0.906851 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.914552, det = 0.962426, fa = 0.132100, cut = -0.200001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.910893, det = 0.944507, fa = 0.122112, cut = -0.040001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.905062, det = 0.921231, fa = 0.110964, cut = 0.199999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.906695, det = 0.967157, fa = 0.151815, cut = 0.074999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.921158, det = 0.947035, fa = 0.104359, cut = 0.139999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.907448, det = 0.913930, fa = 0.099010, cut = 0.334999\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 2> (etBin=1,etaBin=3) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 0.994676 SP (val) = 0.867865 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.21472 SP (val) = 0.905876 stops = 0\n",
      "PatternRecognition                      INFO Epoch 90: mse (train) = 0.194168 SP (val) = 0.905546 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.927381, det = 0.963785, fa = 0.108322, cut = 0.019999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.913000, det = 0.962627, fa = 0.135314, cut = -0.005001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.927381, det = 0.963785, fa = 0.108322, cut = 0.019999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.913000, det = 0.962627, fa = 0.135314, cut = -0.005001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.925844, det = 0.955183, fa = 0.103038, cut = 0.114999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.906469, det = 0.889015, fa = 0.075908, cut = 0.494999\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 3> (etBin=1,etaBin=3) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.06083 SP (val) = 0.849848 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.229376 SP (val) = 0.905855 stops = 1\n",
      "PatternRecognition                      INFO Epoch 94: mse (train) = 0.212315 SP (val) = 0.905392 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.923614, det = 0.950656, fa = 0.103038, cut = 0.034999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.914252, det = 0.944507, fa = 0.115512, cut = 0.034999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.924060, det = 0.951562, fa = 0.103038, cut = 0.134999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.905855, det = 0.979615, fa = 0.165016, cut = -0.265001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.923614, det = 0.949298, fa = 0.101717, cut = 0.089999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.913134, det = 0.935447, fa = 0.108911, cut = 0.134999\n",
      "Py.TuningJob                            INFO Saving file named nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0001.eta0003.pic...\n",
      "Py.TuningJob                            INFO File \"nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0001.eta0003.pic.gz\" saved!\n",
      "Py.TuningJob                            INFO Finished tuning job!\n",
      "Py.TuningJob                            INFO Opening data (etBin=2,etaBin=1) ...\n",
      "Py.TuningJob                            INFO Tunning Et bin: array([    50.,  20000.], dtype=float32)\n",
      "Py.TuningJob                            INFO Tunning eta bin: array([ 0.80000001,  1.37      ], dtype=float32)\n",
      "Py.TuningJob                            INFO Running configuration file number 0 (etBin=2,etaBin=1) \n",
      "Py.TuningJob                            INFO Extracting cross validation sort 0 (etBin=2,etaBin=1) \n",
      "Py.CrossValid                           INFO Train      #Events/class: [261, 188]\n",
      "Py.CrossValid                           INFO Validation #Events/class: [173, 124]\n",
      "Py.TuningJob                            INFO Tuning pre-processing chain (Norm1)...\n",
      "Py.TuningJob                            INFO Applying pre-processing chain...\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 0> (etBin=2,etaBin=1) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.02319 SP (val) = 0.945049 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.0212766 SP (val) = 0.94341 stops = 1\n",
      "PatternRecognition                      INFO Epoch 95: mse (train) = 0.0212766 SP (val) = 0.94341 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.971752, det = 0.972350, fa = 0.028846, cut = 0.399999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.955130, det = 1.000000, fa = 0.088710, cut = -0.940000\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.960028, det = 0.974654, fa = 0.054487, cut = 0.314999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.946855, det = 1.000000, fa = 0.104839, cut = -0.105001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.973137, det = 0.981567, fa = 0.035256, cut = 0.849999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.943252, det = 0.919075, fa = 0.032258, cut = 0.999998\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 1> (etBin=2,etaBin=1) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 0.997856 SP (val) = 0.935739 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.0243669 SP (val) = 0.951988 stops = 1\n",
      "PatternRecognition                      INFO Epoch 65: mse (train) = 0.0224089 SP (val) = 0.949133 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.972673, det = 0.983871, fa = 0.038462, cut = 0.154999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.955130, det = 1.000000, fa = 0.088710, cut = -0.880000\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.972673, det = 0.983871, fa = 0.038462, cut = 0.154999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.955130, det = 1.000000, fa = 0.088710, cut = -0.880000\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.955449, det = 0.965438, fa = 0.054487, cut = 0.044999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.941222, det = 0.988439, fa = 0.104839, cut = -0.045001\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 2> (etBin=2,etaBin=1) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.00665 SP (val) = 0.931912 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.0213829 SP (val) = 0.949473 stops = 2\n",
      "PatternRecognition                      INFO Epoch 61: mse (train) = 0.0212768 SP (val) = 0.945049 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.966688, det = 0.981567, fa = 0.048077, cut = -0.010001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.956421, det = 0.994220, fa = 0.080645, cut = -0.515000\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.962157, det = 0.995392, fa = 0.070513, cut = -0.120001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.952304, det = 0.994220, fa = 0.088710, cut = -0.140001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.950615, det = 0.949309, fa = 0.048077, cut = -0.170001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.934370, det = 1.000000, fa = 0.129032, cut = -0.215001\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 3> (etBin=2,etaBin=1) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.08203 SP (val) = 0.945049 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.0239428 SP (val) = 0.954402 stops = 1\n",
      "PatternRecognition                      INFO Epoch 69: mse (train) = 0.0212893 SP (val) = 0.951988 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.978187, det = 0.988479, fa = 0.032051, cut = -0.640000\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.960141, det = 0.976879, fa = 0.056452, cut = -0.640000\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.971990, det = 0.979263, fa = 0.035256, cut = 0.224999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.952304, det = 0.994220, fa = 0.088710, cut = -0.705000\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.975430, det = 0.986175, fa = 0.035256, cut = -0.265001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.957274, det = 0.971098, fa = 0.056452, cut = -0.265001\n",
      "Py.TuningJob                            INFO Saving file named nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0002.eta0001.pic...\n",
      "Py.TuningJob                            INFO File \"nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0002.eta0001.pic.gz\" saved!\n",
      "Py.TuningJob                            INFO Finished tuning job!\n",
      "Py.TuningJob                            INFO Opening data (etBin=2,etaBin=2) ...\n",
      "Py.TuningJob                            INFO Tunning Et bin: array([    50.,  20000.], dtype=float32)\n",
      "Py.TuningJob                            INFO Tunning eta bin: array([ 1.37      ,  1.53999996], dtype=float32)\n",
      "Py.TuningJob                            INFO Running configuration file number 0 (etBin=2,etaBin=2) \n",
      "Py.TuningJob                            INFO Extracting cross validation sort 0 (etBin=2,etaBin=2) \n",
      "Py.CrossValid                           INFO Train      #Events/class: [44, 30]\n",
      "Py.CrossValid                           INFO Validation #Events/class: [29, 20]\n",
      "Py.TuningJob                            INFO Tuning pre-processing chain (Norm1)...\n",
      "Py.TuningJob                            INFO Applying pre-processing chain...\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 0> (etBin=2,etaBin=2) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.0445 SP (val) = 0.898275 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0 SP (val) = 0.923476 stops = 1\n",
      "PatternRecognition                      INFO Epoch 65: mse (train) = 0 SP (val) = 0.923476 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.932560, det = 0.945205, fa = 0.080000, cut = 0.059999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.957743, det = 0.965517, fa = 0.050000, cut = 0.059999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.979898, det = 1.000000, fa = 0.040000, cut = -0.380001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.949341, det = 1.000000, fa = 0.100000, cut = -0.380001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.918904, det = 0.917808, fa = 0.080000, cut = -0.010001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.929755, det = 0.862069, fa = 0.000000, cut = -0.000001\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 1> (etBin=2,etaBin=2) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 0.999825 SP (val) = 0.898275 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0 SP (val) = 0.957743 stops = 1\n",
      "PatternRecognition                      INFO Epoch 58: mse (train) = 0 SP (val) = 0.957743 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.963012, det = 0.986301, fa = 0.060000, cut = 0.114999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.974840, det = 1.000000, fa = 0.050000, cut = 0.114999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.963012, det = 0.986301, fa = 0.060000, cut = 0.114999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.974840, det = 1.000000, fa = 0.050000, cut = 0.114999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.917884, det = 0.876712, fa = 0.040000, cut = 0.019999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.911753, det = 0.827586, fa = 0.000000, cut = 0.019999\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 2> (etBin=2,etaBin=2) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 0.999398 SP (val) = 0.940493 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0 SP (val) = 0.949341 stops = 2\n",
      "PatternRecognition                      INFO Epoch 58: mse (train) = 0 SP (val) = 0.949341 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.969768, det = 1.000000, fa = 0.060000, cut = -0.000001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.974840, det = 1.000000, fa = 0.050000, cut = -0.000001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.969768, det = 1.000000, fa = 0.060000, cut = -0.000001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.974840, det = 1.000000, fa = 0.050000, cut = -0.000001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.942601, det = 0.945205, fa = 0.060000, cut = 0.449999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.940493, det = 0.931035, fa = 0.050000, cut = 0.449999\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 3> (etBin=2,etaBin=2) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.03223 SP (val) = 0.856498 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0 SP (val) = 0.923476 stops = 2\n",
      "PatternRecognition                      INFO Epoch 57: mse (train) = 0 SP (val) = 0.923476 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.983148, det = 0.986301, fa = 0.020000, cut = 0.089999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.974840, det = 1.000000, fa = 0.050000, cut = 0.064999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.983148, det = 0.986301, fa = 0.020000, cut = 0.089999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.974840, det = 1.000000, fa = 0.050000, cut = 0.064999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.877581, det = 0.780822, fa = 0.020000, cut = -0.180001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.856498, det = 0.724138, fa = 0.000000, cut = -0.180001\n",
      "Py.TuningJob                            INFO Saving file named nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0002.eta0002.pic...\n",
      "Py.TuningJob                            INFO File \"nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0002.eta0002.pic.gz\" saved!\n",
      "Py.TuningJob                            INFO Finished tuning job!\n",
      "Py.TuningJob                            INFO Opening data (etBin=2,etaBin=3) ...\n",
      "Py.TuningJob                            INFO Tunning Et bin: array([    50.,  20000.], dtype=float32)\n",
      "Py.TuningJob                            INFO Tunning eta bin: array([ 1.53999996,  2.5       ], dtype=float32)\n",
      "Py.TuningJob                            INFO Running configuration file number 0 (etBin=2,etaBin=3) \n",
      "Py.TuningJob                            INFO Extracting cross validation sort 0 (etBin=2,etaBin=3) \n",
      "Py.CrossValid                           INFO Train      #Events/class: [358, 227]\n",
      "Py.CrossValid                           INFO Validation #Events/class: [239, 151]\n",
      "Py.TuningJob                            INFO Tuning pre-processing chain (Norm1)...\n",
      "Py.TuningJob                            INFO Applying pre-processing chain...\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 0> (etBin=2,etaBin=3) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.03107 SP (val) = 0.957803 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.063078 SP (val) = 0.961711 stops = 1\n",
      "PatternRecognition                      INFO Epoch 86: mse (train) = 0.0572173 SP (val) = 0.960424 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.975077, det = 0.989950, fa = 0.039683, cut = -0.255001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.973330, det = 1.000000, fa = 0.052980, cut = -0.560000\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.975077, det = 0.989950, fa = 0.039683, cut = -0.255001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.973330, det = 1.000000, fa = 0.052980, cut = -0.560000\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.961785, det = 0.998325, fa = 0.074074, cut = -0.705000\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.966319, det = 0.979079, fa = 0.046358, cut = -0.050001\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 1> (etBin=2,etaBin=3) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.04309 SP (val) = 0.938121 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.0780778 SP (val) = 0.962983 stops = 0\n",
      "PatternRecognition                      INFO Epoch 100: mse (train) = 0.0829661 SP (val) = 0.956293 stops = 3\n",
      "PatternRecognition                      INFO Epoch 103: mse (train) = 0.0651745 SP (val) = 0.956293 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.969229, det = 0.991625, fa = 0.052910, cut = -0.045001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.971265, det = 0.995816, fa = 0.052980, cut = -0.045001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.973080, det = 0.983250, fa = 0.037037, cut = 0.114999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.963232, det = 1.000000, fa = 0.072848, cut = -0.670000\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.973575, det = 0.981575, fa = 0.034392, cut = 0.239999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.966319, det = 0.979079, fa = 0.046358, cut = -0.000001\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 2> (etBin=2,etaBin=3) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.00665 SP (val) = 0.955002 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.077911 SP (val) = 0.959641 stops = 0\n",
      "PatternRecognition                      INFO Epoch 84: mse (train) = 0.0488467 SP (val) = 0.956749 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.971396, det = 0.993300, fa = 0.050265, cut = -0.265001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.968396, det = 0.983264, fa = 0.046358, cut = 0.039999\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.967378, det = 0.993300, fa = 0.058201, cut = -0.220001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.966604, det = 1.000000, fa = 0.066225, cut = -0.355001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.965724, det = 0.989950, fa = 0.058201, cut = -0.160001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.966319, det = 0.979079, fa = 0.046358, cut = 0.259999\n",
      "Py.TuningJob                            INFO Training <Neuron = 16, sort = 0, init = 3> (etBin=2,etaBin=3) ...\n",
      "PatternRecognition                      INFO Epoch 0: mse (train) = 1.00804 SP (val) = 0.922998 stops = 0\n",
      "PatternRecognition                      INFO Epoch 50: mse (train) = 0.0858095 SP (val) = 0.965845 stops = 0\n",
      "PatternRecognition                      INFO Epoch 77: mse (train) = 0.0657479 SP (val) = 0.961711 stops = 4\n",
      "TuningToolPyWrapper                     INFO Maximum number of failures reached. Finishing training...\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.971396, det = 0.993300, fa = 0.050265, cut = -0.105001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.971265, det = 0.995816, fa = 0.052980, cut = -0.120001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.971396, det = 0.993300, fa = 0.050265, cut = -0.105001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.971265, det = 0.995816, fa = 0.052980, cut = -0.120001\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.963096, det = 0.976549, fa = 0.050265, cut = 0.144999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.965481, det = 0.970711, fa = 0.039735, cut = 0.304999\n",
      "Py.TuningJob                            INFO Saving file named nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0002.eta0003.pic...\n",
      "Py.TuningJob                            INFO File \"nn.tuned.ppN1.hn0016.s0000.il0000.iu0003.et0002.eta0003.pic.gz\" saved!\n",
      "Py.TuningJob                            INFO Finished tuning job!\n"
     ]
    }
   ],
   "source": [
    " !runTuning.py -d tuningtoolData.npz \\\n",
    "    --output-level INFO \\\n",
    "    --neuronBounds 16 16 \\\n",
    "    --initBounds 0 4 \\\n",
    "    --sortBounds 0 1 \\\n",
    "    --et-bins 1 2 \\\n",
    "    --eta-bins 1 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Dispatching job to the GRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!runGRIDtuning.py -d user.wsfreund.mc14_13TeV.147406.129160.sgn.offLikelihood.bkg.truth.trig.e24_lhmedium_nod0_l1etcut20_l2etcut19_efetcut24_binned.pic.npz \\\n",
    "                -pp user.jodafons:user.jodafons.Norm1 \\\n",
    "                -c user.wsfreund.config.nn5to20_sorts50_1by1_inits100_100by100_upload \\\n",
    "                -x user.jodafons.crossVal_50sorts_20160302.pic.gz \\\n",
    "                -o user.wsfreund.nn.mc14_13TeV.147406.129160.sgn.offLH.bkg.truth.trig.wf.e24_lhmedium_nod0_l1et20_l2et19_efet24_binned_debug \\\n",
    "                -otar workarea.tgz \\\n",
    "                --debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Standalone job via python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we run a job using 5 initializations of neural networks containing 15 hidden layer neurons. We divide the train and validation dataset using the sort number `0` of the \"default\" CrossValidation object seeded by the integer `66`. The neural network are trained for a total of 100 epochs, the goal is to measure time performance, so the convergence does not avoid over-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Py.__main__                             INFO Entering main job.\n",
      "Py.CrossValid                          DEBUG Retrieved the following configuration:\n",
      "[('nBoxes', 10), ('nSorts', 50), ('nTrain', 6), ('nValid', 4), ('seed', 66)]\n",
      "Py.TuningJob                            INFO Opening data...\n",
      "Py.TuningJob                            INFO Running configuration file number 0\n",
      "Py.TuningJob                            INFO Extracting cross validation sort 0\n",
      "Py.CrossValid                           INFO Train      #Events/class: [11626, 3386]\n",
      "Py.CrossValid                           INFO Validation #Events/class: [7751, 2257]\n",
      "Py.TuningJob                            INFO Tuning pre-processing chain (MapStd)...\n",
      "Py.TuningJob                           DEBUG Done tuning pre-processing chain!\n",
      "Py.TuningJob                            INFO Applying pre-processing chain...\n",
      "Py.TuningJob                           DEBUG Done applying the pre-processing chain!\n",
      "Py.TuningWrapper                       DEBUG Set batchSize to 3386\n",
      "Py.TuningJob                           DEBUG Using validation dataset as test dataset.\n",
      "Py.TuningJob                            INFO Training <Neuron = 15, sort = 0, init = 0>...\n",
      "Py.TuningWrapper                       DEBUG Initalizing newff...\n",
      "Py.TuningWrapper                       DEBUG executing train_c\n",
      "Py.TuningWrapper                       DEBUG finished train_c\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.971314, det = 0.978273, fa = 0.035619, cut = 0.164999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.961926, det = 0.972649, fa = 0.048737, cut = 0.174999\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.964683, det = 0.974248, fa = 0.044834, cut = 0.059999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.958392, det = 0.979487, fa = 0.062472, cut = -0.345001\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.967500, det = 0.973474, fa = 0.038455, cut = 0.039999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.961710, det = 0.967746, fa = 0.044307, cut = 0.129999\n",
      "Py.TuningWrapper                       DEBUG Finished train_c on python side.\n",
      "Py.TuningJob                           DEBUG Finished C++ tuning, appending tuned discriminators to tuning record...\n",
      "Py.TuningJob                            INFO Training <Neuron = 15, sort = 0, init = 1>...\n",
      "Py.TuningWrapper                       DEBUG Initalizing newff...\n",
      "Py.TuningWrapper                       DEBUG executing train_c\n",
      "Py.TuningWrapper                       DEBUG finished train_c\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.972872, det = 0.981937, fa = 0.036151, cut = 0.089999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.961809, det = 0.976003, fa = 0.052282, cut = 0.089999\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.969673, det = 0.980699, fa = 0.041290, cut = 0.029999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.960629, det = 0.981293, fa = 0.059814, cut = -0.220001\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.967686, det = 0.980647, fa = 0.045189, cut = -0.075001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.959576, det = 0.966585, fa = 0.047408, cut = 0.254999\n",
      "Py.TuningWrapper                       DEBUG Finished train_c on python side.\n",
      "Py.TuningJob                           DEBUG Finished C++ tuning, appending tuned discriminators to tuning record...\n",
      "Py.TuningJob                            INFO Training <Neuron = 15, sort = 0, init = 2>...\n",
      "Py.TuningWrapper                       DEBUG Initalizing newff...\n",
      "Py.TuningWrapper                       DEBUG executing train_c\n",
      "Py.TuningWrapper                       DEBUG finished train_c\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.968403, det = 0.978325, fa = 0.041467, cut = 0.094999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.962971, det = 0.971617, fa = 0.045636, cut = 0.224999\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.967520, det = 0.980492, fa = 0.045366, cut = 0.004999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.962608, det = 0.977164, fa = 0.051839, cut = 0.004999\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.970501, det = 0.980750, fa = 0.039695, cut = 0.044999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.961189, det = 0.961811, fa = 0.039433, cut = 0.449999\n",
      "Py.TuningWrapper                       DEBUG Finished train_c on python side.\n",
      "Py.TuningJob                           DEBUG Finished C++ tuning, appending tuned discriminators to tuning record...\n",
      "Py.TuningJob                            INFO Training <Neuron = 15, sort = 0, init = 3>...\n",
      "Py.TuningWrapper                       DEBUG Initalizing newff...\n",
      "Py.TuningWrapper                       DEBUG executing train_c\n",
      "Py.TuningWrapper                       DEBUG finished train_c\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.969713, det = 0.975951, fa = 0.036505, cut = 0.169999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.963317, det = 0.970971, fa = 0.044307, cut = 0.189999\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.961975, det = 0.975435, fa = 0.051391, cut = 0.019999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.956561, det = 0.981680, fa = 0.068232, cut = -0.470001\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.969943, det = 0.978376, fa = 0.038455, cut = 0.074999\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.962728, det = 0.968004, fa = 0.042534, cut = 0.269999\n",
      "Py.TuningWrapper                       DEBUG Finished train_c on python side.\n",
      "Py.TuningJob                           DEBUG Finished C++ tuning, appending tuned discriminators to tuning record...\n",
      "Py.TuningJob                            INFO Training <Neuron = 15, sort = 0, init = 4>...\n",
      "Py.TuningWrapper                       DEBUG Initalizing newff...\n",
      "Py.TuningWrapper                       DEBUG executing train_c\n",
      "Py.TuningWrapper                       DEBUG finished train_c\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Extracted discriminator to raw dictionary.\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.971902, det = 0.984466, fa = 0.040581, cut = -0.100001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.962580, det = 0.980261, fa = 0.054940, cut = -0.100001\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.971832, det = 0.982711, fa = 0.038986, cut = -0.040001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.962324, det = 0.980648, fa = 0.055826, cut = -0.160001\n",
      "Py.TuningWrapper                       DEBUG Retrieving performance.\n",
      "Py.TuningWrapper                        INFO Operation: sp = 0.972490, det = 0.984931, fa = 0.039872, cut = -0.085001\n",
      "Py.TuningWrapper                        INFO Test: sp = 0.961734, det = 0.966456, fa = 0.042977, cut = 0.294999\n",
      "Py.TuningWrapper                       DEBUG Finished train_c on python side.\n",
      "Py.TuningJob                           DEBUG Finished C++ tuning, appending tuned discriminators to tuning record...\n",
      "Py.TuningJob                           DEBUG Finished all initializations for sort 0...\n",
      "Py.TuningJob                           DEBUG Finished all hidden layer neurons for sort 0...\n",
      "Py.TuningJob                           DEBUG Finished all sorts for configuration 0 in collection...\n",
      "Py.TuningJob                            INFO Saving file named nn.tuned.ppstd.hn0015.s0000.il0000.iu0004.no-bin.pic...\n",
      "Py.TuningJob                            INFO File \"nn.tuned.ppstd.hn0015.s0000.il0000.iu0004.no-bin.pic.gz\" saved!\n",
      "Py.TuningJob                            INFO Finished tuning job!\n",
      "Py.__main__                             INFO Finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time is:  93.2350001335\n"
     ]
    }
   ],
   "source": [
    "# %load ../scripts/skeletons/time_test.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# TODO Improve skeleton documentation\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "DatasetLocationInput = '/afs/cern.ch/work/j/jodafons/public/validate_tuningtool/mc14_13TeV.147406.129160.sgn.offLikelihood.bkg.truth.trig.e24_lhmedium_L1EM20VH_etBin_0_etaBin_0.npz'\n",
    "\n",
    "#try:\n",
    "from RingerCore.Logger import Logger, LoggingLevel\n",
    "mainLogger = Logger.getModuleLogger(__name__)\n",
    "mainLogger.info(\"Entering main job.\")\n",
    "\n",
    "from TuningTools.TuningJob import TuningJob\n",
    "tuningJob = TuningJob()\n",
    "\n",
    "from TuningTools.PreProc import *\n",
    "\n",
    "basepath = '/afs/cern.ch/work/j/jodafons/public'\n",
    "\n",
    "tuningJob( DatasetLocationInput, \n",
    "           neuronBoundsCol = [15, 15], \n",
    "           sortBoundsCol = [0, 1],\n",
    "           initBoundsCol = 5, \n",
    "           #confFileList = basepath + '/user.wsfreund.config.nn5to20_sorts50_1by1_inits100_100by100/job.hn0015.s0040.il0000.iu0099.pic.gz',\n",
    "           #ppFileList = basepath+'/user.wsfreund.Norm1/ppFile_pp_Norm1.pic.gz',\n",
    "           #crossValidFile = basepath+'/user.wsfreund.CrossValid.50Sorts.seed_0/crossValid.pic.gz',\n",
    "           epochs = 100,\n",
    "           showEvo = 0,\n",
    "           #algorithmName= 'rprop',\n",
    "           #doMultiStop = True,\n",
    "           #doPerf = True,\n",
    "           maxFail = 100,\n",
    "           #seed = 0,\n",
    "           ppCol = PreProcCollection( PreProcChain( MapStd() ) ),\n",
    "           crossValidSeed = 66,\n",
    "           level = LoggingLevel.DEBUG )\n",
    "\n",
    "mainLogger.info(\"Finished.\")\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print 'execution time is: ', (end - start)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\">\n",
    "    show=true;\n",
    "    function toggle(){\n",
    "        if (show){\n",
    "            $('div.input').hide();\n",
    "        }else{\n",
    "            $('div.input').show();\n",
    "        }\n",
    "        show = !show\n",
    "    }\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
    "</script>\n",
    "<a href=\"javascript:toggle()\" target=\"_self\"></a>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
